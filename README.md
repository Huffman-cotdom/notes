# notes

笔记 notes

TODO
- [ ] LLaMA
	- [ ] Cache causal multi-head attention https://www.zhihu.com/question/596900067/answer/3040011798
- [ ] LLM
	- [ ] https://github.com/lucidrains/MEGABYTE-pytorch
	- [ ] MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers https://arxiv.org/abs/2305.07185
	- [ ] https://github.com/PKU-Alignment/safe-rlhf
	- [ ] https://github.com/jerryjliu/llama_index
	- [ ] NLP2Code https://nl2code.github.io/
	- [ ] Parallel Context Windows for Large Language Models https://arxiv.org/abs/2212.10947
	- [ ] 位置编码：https://zhuanlan.zhihu.com/p/415020704
	- [ ]  关于 ChatGPT 的知识：https://github.com/dalinvip/Awesome-ChatGPT
	- [ ] 关于 ChatGPT 的资源：https://github.com/NicholasCao/Awesome-Chinese-ChatGPT
	- [ ] 用 RLHF 训练 LLaMA 的手把手教程 https://huggingface.co/blog/zh/stackllama
	- [ ] DeepSpeed
	- [ ] A Survey of Large Language Models https://zhuanlan.zhihu.com/p/631065995
	- [ ] GLM：https://zhuanlan.zhihu.com/p/560559133
	- [ ] YuLan-Chat 人民大学在 LLaMA 上做 SFT 训练 https://mp.weixin.qq.com/s/4pk4vHzAf_kiXYWT-abLlg
	- [ ] [TigerBot](https://github.com/TigerResearch/TigerBot)
	- [ ] Alibi（长度外推）：https://arxiv.org/abs/2108.12409
- [ ] [Ladder Side-Tuning：预训练模型的“过墙梯”](https://kexue.fm/archives/9138)