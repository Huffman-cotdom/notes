# notes

笔记 notes

TODO
- [ ] LLaMA
	- [ ] Cache causal multi-head attention https://www.zhihu.com/question/596900067/answer/3040011798
- [ ] LLM
	- [ ] https://github.com/lucidrains/MEGABYTE-pytorch
	- [ ] MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers https://arxiv.org/abs/2305.07185
	- [ ] https://github.com/PKU-Alignment/safe-rlhf
	- [ ] https://github.com/jerryjliu/llama_index
	- [ ] Parallel Context Windows for Large Language Models https://arxiv.org/abs/2212.10947
	- [ ]  关于 ChatGPT 的知识：https://github.com/dalinvip/Awesome-ChatGPT
	- [ ] 关于 ChatGPT 的资源：https://github.com/NicholasCao/Awesome-Chinese-ChatGPT
	- [ ] 用 RLHF 训练 LLaMA 的手把手教程 https://huggingface.co/blog/zh/stackllama
	- [ ] DeepSpeed
	- [ ] A Survey of Large Language Models https://zhuanlan.zhihu.com/p/631065995
	- [ ] GLM：https://zhuanlan.zhihu.com/p/560559133
	- [ ] YuLan-Chat 人民大学在 LLaMA 上做 SFT 训练 https://mp.weixin.qq.com/s/4pk4vHzAf_kiXYWT-abLlg
	- [ ] [TigerBot](https://github.com/TigerResearch/TigerBot)
	- [ ] Prompt tuning
	- [ ] prefix tuning
	- [ ] adapter
	- [ ] LLaMA adapter
	- [ ] Bloom 
	- [ ] mixture-of-experts(MoE)
	- [ ] state-space
	- [ ] Megatron-LM: https://arxiv.org/abs/1909.08053
	- [ ] ZeRO：https://arxiv.org/abs/1910.02054
	- [ ] Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model
	  https://arxiv.org/abs/2201.11990
- [ ] [Ladder Side-Tuning：预训练模型的“过墙梯”](https://kexue.fm/archives/9138)