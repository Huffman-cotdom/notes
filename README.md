# notes

笔记 notes

TODO
- [ ] LLaMA
	- [ ] Cache causal multi-head attention
- [ ] LLM
	- [ ] https://github.com/lucidrains/MEGABYTE-pytorch
	- [ ] MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers https://arxiv.org/abs/2305.07185
	- [ ] https://github.com/PKU-Alignment/safe-rlhf
	- [ ] https://github.com/jerryjliu/llama_index
	- [ ] NLP2Code https://nl2code.github.io/
	- [ ] Parallel Context Windows for Large Language Models https://arxiv.org/abs/2212.10947
	- [x] searchGPT: https://github.com/michaelthwan/searchGPT
	- [ ] 位置编码： https://zhuanlan.zhihu.com/p/415020704
	- [x] C-Eval 中文基准测试： https://yaofu.notion.site/C-Eval-6b79edd91b454e3d8ea41c59ea2af873
	- [ ]  关于 ChatGPT 的知识： https://github.com/dalinvip/Awesome-ChatGPT
	- [ ] 关于 ChatGPT 的资源： https://github.com/NicholasCao/Awesome-Chinese-ChatGPT
	- [ ] 用 RLHF 训练 LLaMA 的手把手教程 https://huggingface.co/blog/zh/stackllama