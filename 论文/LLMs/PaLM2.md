

重点：

- 谷歌内部文件显示，PaLM 经过了 5400 亿的参数训练，而新推出的 PaLM2，训练参数接近腰斩，只有 3400 亿个。

- 但是在另一个模型训练的关键数据——训练语料大小上，谷歌开始疯狂堆料，把 PaLM 的 7800 亿的训练 token 量直接推到了 3.6 万亿！

- 而且除了 Token 数量的激增，PaLM2 在数据质量上也有很大的提升。



Pa LM 2 预训练语料库由多种来源组成：网络文档、书籍、代码、数学、和对话数据。

训练前的语料库明显大于用于训练 Pa LM 的语料库。PaLM2 在包含比以前的大型语言模型更高比例的非英语数据的数据集上进行训练，这有利于多语言任务（例如，翻译和多语言问答）。因为该模型接触到更广泛的语言和文化。这允许模型学习每种语言的细微差别。

除了非英语单语数据外，PaLM2 还接受了涵盖数百种语言的平行数据的训练，这些平行数据以源文本和目标文本对的形式出现，其中一侧是英语。包含并行多语言数据进一步提高了模型理解和生成多语言文本的能力。它还根深蒂固地转化为模型的内在能力，这对各种任务都很有用。没有应用任何过滤来明确保留或删除任何语言。

采用了多种数据清理和质量过滤方法，包括去重、去除敏感和过滤。尽管 PaLM2 的英语数据比例小于 PaLM，但仍然观察到英语评估数据集的显着改进，将这部分归因于 PaLM2 更高的数据质量。

对于一小部分预训练数据，使用来自固定版本的 Perspective API 的信号添加了标记文本毒性的特殊控制标记。重要的是，评估表明控制令牌不会对不相关任务的性能产生负面影响。



