# Less Is More for Alignment

> 论文地址：https://arxiv.org/pdf/2305.11206.pdf

Meta 发布了 LIMA 模型，在 LLaMA-65B 的基础上，只用 1000 个精心准备的样本数据进行微调，无需 RLHF，就达到了和 GPT-4 相媲美的程度。核心思想就是对于一个强大的 LLM，在它预训练的时候就已经学到了绝大部分知识，后续只需要一些精心设计数据微调就足以让模型产生高质量的内容。不需要搞 RLHF 哪些复杂的东西。所以： **Less Is More**！

心得：
- 微调数据量不需要太多：论文只用了 1000 个，实验增加数据量几乎没有变化
- 微调数据质量需要比较高：800 个社区问答数据经过清洗或者手动改写；200 个人工手写。
- 数据需要更加多样性：数据来自不同领域、不同任务。
- 数据风格统一：所有数据都被人工修改为 AI 助手的风格。
- 模型具有泛化能力：在外部分布实验、多轮对话实验中都可以发现模型具有泛化能力。
- 大模型几乎所有的知识都是在预训练过程中学习，只需要有限的指导调整数据就足以教会模型产生高质量的输出。
存在的问题：
- 构建这样的示例需要相当大的心智努力，并且很难扩展。
- LIMA 不像产品级模型那样稳健；虽然 LIMA 通常生成良好的回答，但在解码过程中的不幸样本或对抗性提示往往会导致回答较弱。

## 摘要

大型语言模型的训练分为两个阶段：（1）无监督预训练从原始文本中学习通用表示；（2）大规模的指导调整和强化学习，以更好地与最终任务和用户偏好相匹配。

LIMA 仅仅从训练数据中的少数示例中学会了遵循特定的回复格式，包括从制定旅行行程到推测替代历史的复杂查询。此外，该模型能很好地推广到训练数据中没有出现的未见任务。

**LIMA 的回复在 43％的情况下要么与 GPT-4 相等，要么明显优于 GPT-4**；与 Bard 相比，这个统计数字高达 58％；与经过人类反馈训练的 DaVinci003 相比，这个统计数字高达 65％。**综合这些结果强烈表明，大型语言模型中几乎所有的知识都是在预训练过程中学习的，只需要有限的指导调整数据就足以教会模型产生高质量的输出。**

## 数据

我们提出了表面对齐假设：**模型的知识和能力几乎完全是在预训练期间学习的，而对齐则教会它在与用户互动时应该使用哪种子分布的格式**。如果这个假设是正确的，并且对齐主要是关于学习风格，那么表面对齐假设的一个推论是，可以用相当小的一组示例对预训练的语言模型进行充分调整[Kirstain 等，2021]。

为此，我们收集了 1,000 个提示和回复的数据集，其中输出（回复）在**风格上是相互一致的，但输入（提示）是多样的**。具体而言，我们寻求以有益的 AI 助手的风格输出。我们从各种来源策划这些示例，主要分为社区问答论坛和手动编写的示例。我们还收集了一个包含 300 个提示的测试集和一个包含 50 个提示的开发集。表 1 显示了不同数据源的概览和一些统计信息（有关一些训练示例的选择，请参见附录 A）。

![](assents/Pasted%20image%2020230526132249.png)

### 社区问答

从三个社区问答网站收集数据：Stack Exchange、wikiHow 和 Pushshift Reddit 数据集。大体上说，Stack Exchange 和 wikiHow 的回答与有益的 AI 助理的行为非常一致，因此可以**自动挖掘**，而 Reddit 的高赞回答往往是幽默或恶搞的，**需要更多的手动方式来策划符合适当风格的回复**。

Stack Exchange Stack Exchange 包含 179 个在线社区（交流），我们将交流划分为 75 个 STEM 交流（包括编程、数学、物理等）和 99 个其他交流（英语、烹饪、旅行等）；我们放弃了 5 个小众交流。然后，我们在每个集合中使用温度为 t=3 的采样方式，从每个集合中取出 200 个问题和答案，以**获得不同领域的更均匀的样本**。在每个交流中，我们选择标题自成一体（没有正文）的得分最高的问题。然后，我们选择每个问题的最佳答案，假设它得到了很高的正分（至少 10 分）。**为了符合有益的 AI 助手的风格，我们自动过滤了过短（少于 1200 个字符）、过长（超过 4096 个字符）、以第一人称（“我”，“我的”）写作的回答，以及引用其他回答（“如上所述”，“交流”等）的回答；我们还删除回复中的链接、图片和其他 HTML 标记，仅保留代码块和列表**。由于 Stack Exchange 的问题包含标题和描述，我们随机选择标题作为某些示例的提示，描述作为其他示例的提示。

wikiHow wikiHow 是一个在线的维基风格出版物，涵盖了各种主题的 24 万多个如何做的文章。任何人都可以为 wikiHow 做出贡献，尽管文章经过严格的审核，结果是几乎普遍具有高质量的内容。我们从 wikiHow 中抽样了 200 篇文章，首先抽样一个类别（共有 19 个），然后在该类别中抽样一篇文章，以确保多样性。我们将标题作为提示（例如“如何煮煎蛋？”），将文章正文作为回答。我们将典型的“这篇文章...”开头替换为“The following answer...”，并应用一些预处理启发式方法来修剪链接、图片和文本的某些部分。

Pushshift Reddit 数据集 Reddit 是世界上最受欢迎的网站之一，允许用户在用户创建的子论坛中分享、讨论和赞同内容。由于 Reddit 的广泛受欢迎程度，其更多偏向于娱乐用户，而不是帮助用户；经常出现幽默、讽刺的评论比认真、信息丰富的评论更容易获得更多投票。因此，我们将样本限制在两个子集 r/AskReddit 和 r/WritingPrompts 中，并从每个社区中最受欢迎的帖子中手动选择示例。从 r/AskReddit 中，我们找到了 70 个自成一体的提示（仅标题，无正文），我们将其用作测试集，因为最佳答案不一定可靠。WritingPrompts 子论坛包含虚构故事的设定，然后鼓励其他用户对其进行创造性的补充。我们找到了 150 个提示和高质量回答，涵盖了爱情诗歌和短篇科幻故事等主题，并将其添加到训练集中。所有数据实例都是从 Pushshift Reddit 数据集中挖掘得到的。

### 手动编写的示例

为了进一步使我们的数据多样化，超越在线社区用户提出的问题，我们从我们自己（本文的作者）那里收集提示。我们指定了 A 组和 B 组两组作者，每组创建 250 个提示，灵感来自于他们自己的兴趣或朋友的兴趣。我们从 A 组选择了 200 个提示作为训练集，以及 50 个提示作为开发集。在过滤掉一些问题提示之后，我们从 B 组剩下的 230 个提示中选择了用于测试的提示。我们通过自己编写高质量的回答来补充这 200 个训练提示。在撰写答案时，我们尽量确保统一的语气，以适应一个有益的 AI 助手。具体来说，**许多提示将以对问题的一些承认为开头，然后是答案本身**。初步实验显示，**这种一致的格式通常可以提高模型的性能；我们假设它有助于模型形成一系列思路**，类似于“让我们一步一步思考 COT”的提示[Kojima 等，2022；魏等，2022b]。**我们还包含了 13 个训练提示，其中包含一定程度的有害性或恶意**。我们仔细编写回答，部分或完全拒绝命令，并解释为什么助手不会遵守。测试集中还有 30 个类似问题的提示，我们在第 4.3 节中进行了分析。

除了我们手动编写的示例，我们还从 Super-Natural Instructions[Wang 等，2022b]中抽样了 50 个训练示例。具体而言，我们选择了 50 个自然语言生成任务，如**摘要、改写和风格转换**，并从每个任务中随机选择一个示例。我们稍微编辑了其中的一些示例，以符合我们手动示例的风格。尽管潜在用户提示的分布与 Super-Natural Instructions 中的任务分布有所不同，但我们的直觉是，**这个小样本可以增加训练示例的多样性，并可能增加模型的鲁棒性。**

手动创建多样的提示并编写统一风格的丰富回答是费时费力的。尽管一些最近的研究通过蒸馏和其他自动方式避免了手动劳动[Honovich 等，2022；Wang 等，2022a；Taori 等，2023；Chiang 等，2023；Sun 等，2023]，以量取胜而非质取胜，本文探讨了投资多样性和质量的效果。

## 训练 LIMA

从 LLaMa 65B[Touvron 等，2023]开始，我们在我们的 1,000 个示例的对齐训练集上进行微调。为了区分每个发言者（用户和助手），我们在每个话语的末尾引入了一个特殊的对话结束标记（EOT）；该标记起到了停止生成的 EOS 的相同作用，但避免了预训练模型可能将预先存在的 EOS 标记赋予其他任何含义的混淆。

我们遵循标准的微调超参数：使用 AdamW[Loshchilov 和 Hutter，2017]进行 15 个时期的微调，其中β1 = 0.9，β2 = 0.95，权重衰减为 0.1。没有热身步骤，我们将初始学习率设置为 1e-5，并线性衰减到训练结束时的 1e-6。批量大小设置为 32 个示例（较小的模型为 64），并且超过 2048 个标记的文本将被修剪。

与常规做法有一个显著的偏离，即使用残差丢弃；我们遵循 Ouyang 等[2022]的方法，在残差连接上应用丢弃，从底层开始的 pd = 0.0，线性增加到最后一层的 pd = 0.3（较小的模型为 pd = 0.2）。

我们发现困惑度与生成质量没有相关性，因此在 50 个开发集示例上手动选择第 5 至第 10 个 epochs 之间的检查点。

## 人类评估

### 实验设置

为了将 LIMA 与其他模型进行比较，我们为每个测试提示生成一条回复。然后，我们请众包工人将 LIMA 的输出与每个基准模型进行比较，并标记他们更喜欢哪个。

我们重复这个实验，用 GPT-4 代替人类众包工人，并发现类似的一致性水平。

#### Benchmark 模型

Alpaca 65B：52000 个训练集对 LLaMA 进行微调

OpenAI 的 DaVinci003：这是一个经过强化学习从人类反馈中调优的大型语言模型

Google 的 Bard：基于 PaLM

Anthropic 的 Claude：这是一个使用强化学习从 AI 反馈中训练的 520B 参数模型

OpenAI 的 GPT-4

#### 生成

对于每个提示，我们使用核心采样方法[Holtzman 等，2019]生成每个基准模型的一条回答，其中ρ = 0.9，温度τ = 0.7。我们对先前生成的标记进行重复惩罚，惩罚系数为 1.2[Keskar 等，2019]。我们将最大标记长度限制为 2048。

#### 方法

在每个步骤中，我们向注释者展示一个提示和两个可能的回答，这些回答是由不同的模型生成的。我们要求注释者标记哪个回答更好，或者是否两个回答都没有明显优于另一个。附录 C 提供了确切的措辞。我们通过向 GPT-4 提供完全相同的指令和数据来收集并行注释。

#### 注释者之间的一致性

我们使用调整过的准确率来计算注释者之间的一致性：如果两个注释者都同意，分配 1 分；如果任何一个注释者（但不是两个）标记为平局，分配 0.5 分；否则，分配 0 分。我们在一个共享的 50 个注释示例集合上测量一致性（单个提示，两个模型回答 - 所有随机选择），比较了作者、众包和 GPT-4 的注释。在人类注释者中，我们得到以下一致性分数：众包 - 众包 82％，众包 - 作者 81％，作者 - 作者 78％。尽管这个任务存在一定程度的主观性，但人类注释者之间存在较好的一致性。

我们还测量了 GPT-4 与人类之间的一致性：众包-GPT 78％，作者-GPT 79％（尽管我们使用随机解码，但 GPT-4 几乎总是与自身一致）。这些数字将 GPT-4 与人类注释者的一致性相提并论，实质上通过了这项任务的 Turking 测试[Efrat 和 Levy，2020]。

### 实验结果

![](assents/截屏2023-05-26%2011.01.53.png)

### 分析

我们的主要评估是将 LIMA 与最先进的模型进行比较，但必须记住，**其中一些基准（估计是说的 Alpaca）模型实际上是经过高度调优的产品，在训练过程中可能接触到数百万真实用户的提示，这创造了一个非常高的标准**。

因此，我们通过手动分析 50 个随机示例来提供绝对评估。我们将每个示例分为三个类别之一：失败（Fail），回答未满足提示的要求；通过（Pass），回答满足提示的要求；优秀（Excellent），模型对提示提供了出色的回答。

![](assents/Pasted%20image%2020230526125500.png)

50% 的 LIMA 回答被认为是优秀的，并且在分析的 50 个提示中，44 个能够遵循所有的要求。6 个回答失败，我们没有观察到任何显著的趋势。

#### 外部分布

在分析的 50 个例子中，有 43 个与训练集中的某个样本在格式上有些相关（例如问答、建议、写信等）。

我们分析了额外的 13 个分布外例子（总共 20 个），发现 20% 的回答失败，35% 的回答通过，45% 的回答优秀。尽管这只是一个小样本，但 LIMA 在其训练分布之外似乎实现了类似的绝对性能统计，表明其能够**良好地进行泛化**。图 4 显示了当要求 LIMA 写喜剧段子或点披萨时，它的反应。

![](assents/Pasted%20image%2020230526125945.png)
#### 安全性

最后，我们分析了在训练集中只包含少量与安全相关的例子（仅 13 个；参见第 2.2）的影响。我们检查 LIMA 对测试集中 30 个潜在敏感提示的回应，**发现 LIMA 对其中 80% 的回应是安全的（包括 10 个具有恶意意图的提示中的 6 个）**。在某些情况下，LIMA 直接拒绝执行任务（例如要求提供名人的地址），但当恶意意图是隐含的时，LIMA 更有可能提供不安全的回应，如图 4 所示


## 消融实验：对数据多样性、质量和数量的消融实验

我们通过消融实验来研究训练数据的多样性、质量和数量对结果的影响。我们观察到，在对齐的目的下，扩大输入多样性和输出质量会产生可衡量的积极效果，而仅仅增加数量可能并不会产生同样的效果。

### 实验设置

我们在各种数据集上对一个 7B 参数的 LLaMa 模型[Touvron et al., 2023]进行微调，控制相同的超参数（第 3 节）。然后，我们为每个测试集提示抽样生成 5 个回答，并通过询问 ChatGPT（GPT-3.5 Turbo）评估回答的有用性，使用 1-6 个 Likert 量表进行打分（详见附录 D 中的模板）。我们报告平均分数，并附上一个 p = 0.95 的双侧置信区间。

![](assents/Pasted%20image%2020230526130631.png)

### 多样性

为了测试提示多样性的影响，同时控制质量和数量，我们比较了在经过质量过滤的 Stack Exchange 数据和在具有优秀回答的同质性提示的 wikiHow 数据上进行训练的效果。虽然我们将 Stack Exchange 与 wikiHow 进行比较作为多样性的代理，但我们承认从两个不同来源的数据进行采样时可能存在其他混淆因素。我们从每个来源抽样了 2,000 个训练样本（遵循第 2.1 节的相同协议）。图 5 显示，**更多样化的 Stack Exchange 数据显著提高了性能。**

### 质量

为了测试回答质量的影响，我们从 Stack Exchange 中抽样了 2,000 个例子，没有应用任何质量或风格的过滤，并将在这个数据集上训练的模型与在我们过滤后的数据集上训练的模型进行比较。图 5 显示，**在经过过滤和未经过滤的数据源上训练的模型之间存在显著的 0.5 分差异。**

### 数量

在许多机器学习设置中，扩大样本数量是改善性能的一种众所周知的策略。为了测试其对我们设置的影响，我们从 Stack Exchange 中指数级增加训练集的大小。图 6 显示，令人惊讶的是，将训练集翻倍并没有提高回答质量。这个结果与本节中的其他发现一起表明，**对齐的扩展规律可能不仅仅取决于数量，而是与提示的多样性以及保持高质量回答的功能相关。**

## 多轮对话

一个仅在 1,000 个单轮交互中进行微调的模型能够参与多轮对话吗？

我们在 10 个实时对话中测试了 LIMA，并将每个回答标记为失败（Fail）、通过（Pass）或优秀（Excellent）（详见第 4.3 节）。对于一个零样本聊天机器人来说，LIMA 的回答出奇地连贯，引用了对话中先前步骤的信息。

**很明显，模型正在进行超出其训练分布的操作（没有多轮对话训练数据，却拥有多轮对话能力**。

在 10 个对话中，LIMA 在 3 轮交互内未能遵循提示。

![](assents/Pasted%20image%2020230526131051.png)
为了提高其对话能力，我们收集了**30 个多轮对话链。其中，10 个对话是由作者撰写的，而剩下的 20 个则基于 Stack Exchange 的评论链**，我们对其进行了**编辑以适应助手的风格**。我们使用合并后的 1,030 个示例对预训练的 LLaMa 模型进行了新版本的微调，并根据与零样本模型使用相同的提示进行了 10 个实时对话。图 8 显示了这些对话的摘录。

图 7 显示了回答质量的分布情况。**添加对话显著提高了生成质量**，使优秀回答的比例从 45.2% 提高到 76.1%。此外，失败率从 42 轮中的 15 次（零样本）下降到 46 轮中的 1 次（微调）。我们进一步比较了整个对话的质量，并发现在 10 个对话中，微调模型在 7 个对话中明显更好，并与零样本模型在 3 个对话中持平。从仅仅 30 个示例到达如此能力的飞跃，以及零样本模型能够进行对话的事实，强化了这样的假设，**即这些能力是在预训练过程中学习的，并可以通过有限的监督调用出来**。

![](assents/Pasted%20image%2020230526131203.png)
## 讨论

我们展示了在 1,000 个精心筛选的示例上对强大的预训练语言模型进行微调可以在广泛的提示上产生出色且有竞争力的结果。

然而，这种方法存在一些限制。
首先，构建这样的示例需要相当大的心智努力，并且很难扩展。
其次，LIMA 不像产品级模型那样稳健；虽然 LIMA 通常生成良好的回答，但在解码过程中的不幸样本或对抗性提示往往会导致回答较弱。
尽管如此，本研究所呈现的证据表明，用简单的方法解决对齐问题具有潜力。